{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in PyTorch\n",
    "\n",
    "In this notebook, I'll construct a character-level RNN with PyTorch. If you are unfamiliar with character-level RNNs, check out [this great article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina, one of my favorite novels. I call this project Anna KaRNNa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('wonderland.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163817\n"
     ]
    }
   ],
   "source": [
    "print (len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the text, encode it as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "print (len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "We're one-hot encoding the data, so I'll make a function to do that.\n",
    "\n",
    "I'll also create mini-batches for training. We'll take the encoded characters and split them into multiple sequences, given by `n_seqs` (also refered to as \"batch size\" in other places). Each of those sequences will be `n_steps` long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to define the architecture of the network. We start by defining the layers and operations we want. Then, define a method for the forward pass. I'm also going to write a method for predicting characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=True, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=True, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.data[0])\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data[0]),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes (number of sequences and number of steps), and start the training. With the train function, we can set the number of epochs, the learning rate, and other parameters. Also, we can run the training on a GPU by setting `cuda=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.3827... Val Loss: 3.4926\n",
      "Epoch: 2/25... Step: 20... Loss: 3.2568... Val Loss: 3.4235\n",
      "Epoch: 3/25... Step: 30... Loss: 3.1053... Val Loss: 3.3331\n",
      "Epoch: 4/25... Step: 40... Loss: 2.9467... Val Loss: 3.2453\n",
      "Epoch: 5/25... Step: 50... Loss: 2.7409... Val Loss: 3.1186\n",
      "Epoch: 6/25... Step: 60... Loss: 2.6168... Val Loss: 3.0659\n",
      "Epoch: 7/25... Step: 70... Loss: 2.5594... Val Loss: 3.0108\n",
      "Epoch: 8/25... Step: 80... Loss: 2.4409... Val Loss: 3.0202\n",
      "Epoch: 9/25... Step: 90... Loss: 2.3996... Val Loss: 2.9838\n",
      "Epoch: 10/25... Step: 100... Loss: 2.3578... Val Loss: 3.0189\n",
      "Epoch: 10/25... Step: 110... Loss: 2.3383... Val Loss: 2.9325\n",
      "Epoch: 11/25... Step: 120... Loss: 2.2398... Val Loss: 2.9489\n",
      "Epoch: 12/25... Step: 130... Loss: 2.2062... Val Loss: 2.9498\n",
      "Epoch: 13/25... Step: 140... Loss: 2.1824... Val Loss: 2.8988\n",
      "Epoch: 14/25... Step: 150... Loss: 2.1611... Val Loss: 2.9384\n",
      "Epoch: 15/25... Step: 160... Loss: 2.1259... Val Loss: 2.9199\n",
      "Epoch: 16/25... Step: 170... Loss: 2.0492... Val Loss: 2.8907\n",
      "Epoch: 17/25... Step: 180... Loss: 2.0938... Val Loss: 2.8855\n",
      "Epoch: 18/25... Step: 190... Loss: 2.0332... Val Loss: 2.8893\n",
      "Epoch: 19/25... Step: 200... Loss: 2.0210... Val Loss: 2.8587\n",
      "Epoch: 20/25... Step: 210... Loss: 2.0226... Val Loss: 2.8384\n",
      "Epoch: 20/25... Step: 220... Loss: 2.0000... Val Loss: 2.8409\n",
      "Epoch: 21/25... Step: 230... Loss: 1.9132... Val Loss: 2.8382\n",
      "Epoch: 22/25... Step: 240... Loss: 1.8976... Val Loss: 2.8215\n",
      "Epoch: 23/25... Step: 250... Loss: 1.9151... Val Loss: 2.8143\n",
      "Epoch: 24/25... Step: 260... Loss: 1.8997... Val Loss: 2.8037\n",
      "Epoch: 25/25... Step: 270... Loss: 1.8639... Val Loss: 2.7932\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best model\n",
    "\n",
    "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it. To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorcial probability distribution over all the possible characters. We can make the sampled text more reasonable but less variable by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text.\n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=True):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play of my sperm stockings and played that time to servants. This woman was then were all sighs. \"I have never as I come to more than shorter wide with that sire to ston,\" and I have seep me it. \"No to,\" said I. \"Would I say,\" said I.\n",
      "\n",
      "I write to see the clitoris whole and time, and all the bealty-cupsion of the freedy woman to the stacks. I was near me. I went to the tip at against of the feelings a cunt. I said she soon heard, as well sat off.\n",
      "\n",
      "Then when I saw her charms. \"Yes and money then had barken, and the compreses a soon to me, but I do something of my prick and show,\" said the girl, and stept it which her sentererse trearle of a word went up the bed whispers to say her cunt; but I dare say she did not leave my mand.\" Twenty-probumed whose seemed motte to man had my prick was a bed.\" The nost short of mine with half out, and her heady was over than our. \"No she's, but she would see me,--a don't mant mother against it, and the woman sent me a boy as it out.\" \"You won't look, and I'm geing tired for me. She and the cousin she would not get him.\" \"Yes,\" said he, \"and all that there, I am in tee off a lenger on her cunt,--then I have said to her, and with the shant of mother. There wored the girl's seeing to have the parlour oper at her thighs, and we were told young,\" said she then she had so that them without maked me a cunt. I would not he said, but I had saying they was tickling me and thinking in the sate time.\n",
      "\n",
      "She had some his parts of her and time the some of the clitoris of her should night, then she setted her belly have told her a will gald them, so that I had been fucking to me, to him the she dressed at him with my mother again the boddes and to say at the same time with her, and was a fine weak. I treeped; and was it would left him a little contress was so to me, and with her. This talked woman to be sure, I had not handed the long time I was to mind a candle, said I will be a strelm. She did not spend her. It times a woman was too made in my fingers, \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='play', top_k=5, cuda=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 1.8566... Val Loss: 2.7866\n",
      "Epoch: 2/10... Step: 20... Loss: 1.8310... Val Loss: 2.7823\n",
      "Epoch: 3/10... Step: 30... Loss: 1.8317... Val Loss: 2.7753\n",
      "Epoch: 4/10... Step: 40... Loss: 1.8191... Val Loss: 2.7739\n",
      "Epoch: 5/10... Step: 50... Loss: 1.7939... Val Loss: 2.7664\n",
      "Epoch: 6/10... Step: 60... Loss: 1.7335... Val Loss: 2.7586\n",
      "Epoch: 7/10... Step: 70... Loss: 1.7659... Val Loss: 2.7513\n",
      "Epoch: 8/10... Step: 80... Loss: 1.7269... Val Loss: 2.7494\n",
      "Epoch: 9/10... Step: 90... Loss: 1.7318... Val Loss: 2.7351\n",
      "Epoch: 10/10... Step: 100... Loss: 1.7477... Val Loss: 2.7440\n",
      "Epoch: 10/10... Step: 110... Loss: 1.7469... Val Loss: 2.7466\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(loaded, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': loaded.n_hidden,\n",
    "              'n_layers': loaded.n_layers,\n",
    "              'state_dict': loaded.state_dict(),\n",
    "              'tokens': loaded.chars}\n",
    "with open('rnn_best.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said--’\n",
      "\n",
      "‘But was it was not mare a list,’ said Alice, as she said nowing\n",
      "at the wasten in one the\n",
      "that they was gridesed at it tome,’ said the Gryphon: she had all her she\n",
      "see and the tome of a mading,’ said the Mock Turtle and to herself.\n",
      "\n",
      "‘I was she thing to to say, and have it said a wask to sead hind herself, the Dungess the was once tay, way in inther wondes alows has\n",
      "anous, the with a targe wonded of inth head herss of carded of thoust of all antter to she can themerself, they were of she some of hook.\n",
      "\n",
      "Alice somedting anding a the ruse of the\n",
      "shard was she take, ‘and that it say they down’t he sead the come begon ot mise. ‘Whele you gon’t\n",
      "like that,’ said Alice: ‘a last at som,’ said the King.\n",
      "\n",
      "‘And then the shats as thit hard a the chemeres was the to ders ofter, and whise the parser head too hast the stood of the\n",
      "wither,\n",
      "sad in a meather.’\n",
      "\n",
      "‘The was on one and toors all\n",
      "the sood and and ther she was to seard his aster on the outter,’ said Alice.\n",
      "\n",
      "‘That in showe, and she ceartly, and who the grows there as\n",
      "a lentions and heres and said\n",
      "tain. ‘Nhtice of the shat thing that’re to tay the torss itsee of hinded hoored off the sime,\n",
      "      Ad word a litt at the poor of the samare of sild so chenter, ‘It wened to the said of aly some, the whater\n",
      "sally har the somout a little sain.\n",
      "The she hersed, and the seet the bores was some one one who\n",
      "talk to hand.\n",
      "\n",
      "‘When whet you know, and, the Dormoss of all, and sam it\n",
      "on anore,\n",
      "‘and the sarme, surten the sime to see the was ance of the wast wime to be or theness, as the words on one heres for a chingersed her all a grounten of aterstly seed the toors it thing to ster ance. As the pootse, she\n",
      "could the March Hire with a the roush of the taid.\n",
      "\n",
      "There wat no gring a listle\n",
      "back wanted the poor to this hadd and the batther at the will to sead to her hard of all what a gard then she went, and has said thongid the said, and as if a the\n",
      "remarte to ghe the\n",
      "wast that at the radanco,’ said the Hatten: ‘and they ance ander onch ore that,\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, cuda=True, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
