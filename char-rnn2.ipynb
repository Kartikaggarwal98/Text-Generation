{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in PyTorch\n",
    "\n",
    "In this notebook, I'll construct a character-level RNN with PyTorch. If you are unfamiliar with character-level RNNs, check out [this great article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina, one of my favorite novels. I call this project Anna KaRNNa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('wonderland.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163817\n"
     ]
    }
   ],
   "source": [
    "print (len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the text, encode it as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "print (len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "We're one-hot encoding the data, so I'll make a function to do that.\n",
    "\n",
    "I'll also create mini-batches for training. We'll take the encoded characters and split them into multiple sequences, given by `n_seqs` (also refered to as \"batch size\" in other places). Each of those sequences will be `n_steps` long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to define the architecture of the network. We start by defining the layers and operations we want. Then, define a method for the forward pass. I'm also going to write a method for predicting characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=True, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=True, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.data[0])\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data[0]),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes (number of sequences and number of steps), and start the training. With the train function, we can set the number of epochs, the learning rate, and other parameters. Also, we can run the training on a GPU by setting `cuda=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.3827... Val Loss: 3.4926\n",
      "Epoch: 2/25... Step: 20... Loss: 3.2568... Val Loss: 3.4235\n",
      "Epoch: 3/25... Step: 30... Loss: 3.1053... Val Loss: 3.3331\n",
      "Epoch: 4/25... Step: 40... Loss: 2.9467... Val Loss: 3.2453\n",
      "Epoch: 5/25... Step: 50... Loss: 2.7409... Val Loss: 3.1186\n",
      "Epoch: 6/25... Step: 60... Loss: 2.6168... Val Loss: 3.0659\n",
      "Epoch: 7/25... Step: 70... Loss: 2.5594... Val Loss: 3.0108\n",
      "Epoch: 8/25... Step: 80... Loss: 2.4409... Val Loss: 3.0202\n",
      "Epoch: 9/25... Step: 90... Loss: 2.3996... Val Loss: 2.9838\n",
      "Epoch: 10/25... Step: 100... Loss: 2.3578... Val Loss: 3.0189\n",
      "Epoch: 10/25... Step: 110... Loss: 2.3383... Val Loss: 2.9325\n",
      "Epoch: 11/25... Step: 120... Loss: 2.2398... Val Loss: 2.9489\n",
      "Epoch: 12/25... Step: 130... Loss: 2.2062... Val Loss: 2.9498\n",
      "Epoch: 13/25... Step: 140... Loss: 2.1824... Val Loss: 2.8988\n",
      "Epoch: 14/25... Step: 150... Loss: 2.1611... Val Loss: 2.9384\n",
      "Epoch: 15/25... Step: 160... Loss: 2.1259... Val Loss: 2.9199\n",
      "Epoch: 16/25... Step: 170... Loss: 2.0492... Val Loss: 2.8907\n",
      "Epoch: 17/25... Step: 180... Loss: 2.0938... Val Loss: 2.8855\n",
      "Epoch: 18/25... Step: 190... Loss: 2.0332... Val Loss: 2.8893\n",
      "Epoch: 19/25... Step: 200... Loss: 2.0210... Val Loss: 2.8587\n",
      "Epoch: 20/25... Step: 210... Loss: 2.0226... Val Loss: 2.8384\n",
      "Epoch: 20/25... Step: 220... Loss: 2.0000... Val Loss: 2.8409\n",
      "Epoch: 21/25... Step: 230... Loss: 1.9132... Val Loss: 2.8382\n",
      "Epoch: 22/25... Step: 240... Loss: 1.8976... Val Loss: 2.8215\n",
      "Epoch: 23/25... Step: 250... Loss: 1.9151... Val Loss: 2.8143\n",
      "Epoch: 24/25... Step: 260... Loss: 1.8997... Val Loss: 2.8037\n",
      "Epoch: 25/25... Step: 270... Loss: 1.8639... Val Loss: 2.7932\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best model\n",
    "\n",
    "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it. To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorcial probability distribution over all the possible characters. We can make the sampled text more reasonable but less variable by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text.\n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=True):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play, with a lanking a trome as and or, ald to ling harsed.\n",
      "\n",
      " And ther say ous to she had oncouse out as ather an in the wher sould\n",
      "tongen instowe to the had allent\n",
      "of her wanger the soll toor the wely\n",
      "on wo lowe all of the timo, and the coom a corly, and the peated it the salle a louge, and was it had and worning anding, the\n",
      "\n",
      "onch a seat to the\n",
      "semouse of that wong of the them soned and same\n",
      "that,’ said Alice.\n",
      "\n",
      "‘Whet in hers house fith on a latt the\n",
      "say, it and whe on the soor was shat the would a bit\n",
      "aresed of sor thoused the Mick Turtill as the her sam if hars as inder it sulle abling to bet in then\n",
      "it, was that was all was so sald the was some a chiling hown,\n",
      "‘Itll was\n",
      "its whele was allers a lore ald and of that histlle, ‘and to sous of\n",
      "the wandersely and wern with the batting of\n",
      "ther same on in, wond aly wasting and the wall the wall a searin of ther had off oun the wan to the tried, all hear have out at the seed of it her halder husted,\n",
      "‘why was and wink the warse and wert a sary ats the courter anded is anl weat and hing\n",
      "ther she\n",
      "thought\n",
      "and wosh ally and has the cames, the gooked angar of that hers, frean inte this hat has in sald,\n",
      "thit shat she was she soond intely atling of it had in the she had seard one heare with on thing that hersel, ‘What the tith the rouper,’ said the Cateprinair;\n",
      "\n",
      "‘Then winge in tomen shouse, when wish a tore, and she\n",
      "was taing ther was and sance aly agation if a the sambering of the sith thit then was on as angan as the sood of the hard hand she worg\n",
      "ang of has sing, and the time so whow a ting the thars, and she come on shat telles all sto his to her he fore, shard the hadden.\n",
      "‘That some one toon thong!’ said the\n",
      "King, ald suched in a great in\n",
      "and has, and she came thit here to go the thine was hor and allather the sore, an she would\n",
      "on wish a done ald, and and hers wast he way to betines of insoring ther histly the that wele and wonce some and of the hang if.\n",
      "\n",
      "‘I was the moresed with a canly a sele, ‘Whow I do dow too to leven all of \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='play', top_k=5, cuda=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 1.8566... Val Loss: 2.7866\n",
      "Epoch: 2/10... Step: 20... Loss: 1.8310... Val Loss: 2.7823\n",
      "Epoch: 3/10... Step: 30... Loss: 1.8317... Val Loss: 2.7753\n",
      "Epoch: 4/10... Step: 40... Loss: 1.8191... Val Loss: 2.7739\n",
      "Epoch: 5/10... Step: 50... Loss: 1.7939... Val Loss: 2.7664\n",
      "Epoch: 6/10... Step: 60... Loss: 1.7335... Val Loss: 2.7586\n",
      "Epoch: 7/10... Step: 70... Loss: 1.7659... Val Loss: 2.7513\n",
      "Epoch: 8/10... Step: 80... Loss: 1.7269... Val Loss: 2.7494\n",
      "Epoch: 9/10... Step: 90... Loss: 1.7318... Val Loss: 2.7351\n",
      "Epoch: 10/10... Step: 100... Loss: 1.7477... Val Loss: 2.7440\n",
      "Epoch: 10/10... Step: 110... Loss: 1.7469... Val Loss: 2.7466\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(loaded, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': loaded.n_hidden,\n",
    "              'n_layers': loaded.n_layers,\n",
    "              'state_dict': loaded.state_dict(),\n",
    "              'tokens': loaded.chars}\n",
    "with open('rnn_best.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said--’\n",
      "\n",
      "‘But was it was not mare a list,’ said Alice, as she said nowing\n",
      "at the wasten in one the\n",
      "that they was gridesed at it tome,’ said the Gryphon: she had all her she\n",
      "see and the tome of a mading,’ said the Mock Turtle and to herself.\n",
      "\n",
      "‘I was she thing to to say, and have it said a wask to sead hind herself, the Dungess the was once tay, way in inther wondes alows has\n",
      "anous, the with a targe wonded of inth head herss of carded of thoust of all antter to she can themerself, they were of she some of hook.\n",
      "\n",
      "Alice somedting anding a the ruse of the\n",
      "shard was she take, ‘and that it say they down’t he sead the come begon ot mise. ‘Whele you gon’t\n",
      "like that,’ said Alice: ‘a last at som,’ said the King.\n",
      "\n",
      "‘And then the shats as thit hard a the chemeres was the to ders ofter, and whise the parser head too hast the stood of the\n",
      "wither,\n",
      "sad in a meather.’\n",
      "\n",
      "‘The was on one and toors all\n",
      "the sood and and ther she was to seard his aster on the outter,’ said Alice.\n",
      "\n",
      "‘That in showe, and she ceartly, and who the grows there as\n",
      "a lentions and heres and said\n",
      "tain. ‘Nhtice of the shat thing that’re to tay the torss itsee of hinded hoored off the sime,\n",
      "      Ad word a litt at the poor of the samare of sild so chenter, ‘It wened to the said of aly some, the whater\n",
      "sally har the somout a little sain.\n",
      "The she hersed, and the seet the bores was some one one who\n",
      "talk to hand.\n",
      "\n",
      "‘When whet you know, and, the Dormoss of all, and sam it\n",
      "on anore,\n",
      "‘and the sarme, surten the sime to see the was ance of the wast wime to be or theness, as the words on one heres for a chingersed her all a grounten of aterstly seed the toors it thing to ster ance. As the pootse, she\n",
      "could the March Hire with a the roush of the taid.\n",
      "\n",
      "There wat no gring a listle\n",
      "back wanted the poor to this hadd and the batther at the will to sead to her hard of all what a gard then she went, and has said thongid the said, and as if a the\n",
      "remarte to ghe the\n",
      "wast that at the radanco,’ said the Hatten: ‘and they ance ander onch ore that,\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, cuda=True, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
